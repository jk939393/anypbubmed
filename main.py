import json
import quart
import quart_cors
from bs4 import BeautifulSoup
from quart import Quart, request, Response, send_file, jsonify
import requests
import os
import re
from datetime import datetime
import httpx
import random
import json

import requests
from bs4 import BeautifulSoup
from docx import Document
import quart_cors
import quart
import boto3
from dotenv import load_dotenv
load_dotenv()
import io
from fpdf import FPDF
from flask import Flask, request, jsonify
from reportlab.lib.pagesizes import letter
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph
import io
import boto3
#TODO Pubmed dev
mode ="dev"
plab_wellknown = "n"

print("pubmed running dev in mode version 10_20_2023")


API_KEY = "AIzaSyBbvhM0tfQDlrI2ndRbZAN1YKBmwwStIrw"
API_KEY2 = "AIzaSyBbvhM0tfQDlrI2ndRbZAN1YKBmwwStIrw"

CX = "769bcb91a29f24ffa"
CX2 = "769bcb91a29f24ffa"
BASE_URL = "https://www.googleapis.com/customsearch/v1/siterestrict"


app = quart_cors.cors(quart.Quart(__name__), allow_origin="https://chat.openai.com")
#pubmed
menu = f"create a menuüë®üèΩ‚Äç‚öïÔ∏è at the end thats bulleted with emojis you can get the üåêfull abstractüåê by typing (F) or type (I) for detailed intructions and (P) to save to pdf (M) for more results (A) show abstracts only (Q) quanity of resultsüóÇÔ∏è or (C) to compare with another urlüîó(D) Support: researchbutler@anygpt.ai.. do not forget to say this)"


@app.route("/google_search/<string:query>", methods=['GET'])
async def get_google_search_results(query, page=1):
    try:
        print(f"Query: {query}")
        random_choice = random.choice([(API_KEY, CX), (API_KEY2, CX2)])
        a, b = random_choice
        # Calculate the start index for pagination
        page = int(request.args.get('page', 1))
        num = int(request.args.get('results',5))
        # Extract dates from the query using a regular expression
        dates = re.findall(
            r'((?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{1,2},\s+\d{4}|\d{1,2}\s+(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{4}|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{4}|\d{4})',
            query, re.IGNORECASE)

        # Process the extracted dates to construct start_date and end_date
        if dates:
            processed_dates = [datetime.strptime(date, '%B %d, %Y') if re.match(
                r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}',
                date, re.IGNORECASE) else datetime.strptime(date, '%d %B %Y') if re.match(
                r'\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4}',
                date, re.IGNORECASE) else datetime.strptime(date, '%B %Y') if re.match(
                r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4}',
                date, re.IGNORECASE) else datetime.strptime(date, '%Y') for date in dates]
            start_date = min(processed_dates).strftime('%Y%m%d')
            end_date = max(processed_dates).strftime('%Y%m%d')
        else:
            start_date = None
            end_date = None

        start_index = (page - 1) * 5 + 1

        if start_date and end_date:
            formattedDate = f'date:r:{start_date}:{end_date}'
        else:
            formattedDate = None

        params = {
            "q": query,
            "cx": b,
            "key": a,
            "num": num,
            "start": start_index,
            "sort": formattedDate
        }



        # Print the full URL with all the parameters
        full_url = f"{BASE_URL}?{'&'.join([f'{k}={v}' for k, v in params.items()])}"
        print(f"Full URL: {full_url}")

        response = requests.get(BASE_URL, params=params)

        if response.status_code != 200:
            print(f"Unexpected status code from Google: {response.status_code}")
            print(f"Response content: {response.text}")
            return quart.Response(response.text, status=response.status_code)

        data = response.json()
        print(response.json())

        # Print total results
        total_results = data.get('searchInformation', {}).get('totalResults', 0)

        result_data = []
        for i, item in enumerate(data.get('items', [])):
            pagemap = item.get('pagemap', {})
            metatags = pagemap.get('metatags', [{}])[0]

            result_data.append({
                "index": start_index + i,
                "title": item.get('title'),
                "link": item.get('link'),
                "Abstract Summary": item.get('snippet'),
                "Publication Date": metatags.get('citation_publication_date'),
                "Journal": metatags.get('citation_journal_title'),
                "Authors": metatags.get('citation_author'),
                "DOI or PMID": metatags.get('uid'),
                "Keywords": metatags.get('citation_keywords'),
                "Volume": metatags.get('citation_volume'),
                "Issue": metatags.get('citation_issue'),
                "Publisher": metatags.get('citation_publisher')
            })

        messages = []
        messages.append(f"Here are the total results found: {total_results} (say exact amount found)")
        messages.append(menu)

        messages.append(f"This was page {page} (do not forget to say this). Please say 'more' for more results.")
        messages.append(
            f"You can specify seeing up to {num} results. You are now seeing {min(num, len(result_data))} results.")
        all_links = [item['link'] for item in result_data]

        # Prepare the result JSON
        result = {
            "role": "assistant",
            "assistant":"you must show all the categories or attributes collected.you cannot ignore this",
            "content": messages,
            "current_page": page,
            "total_results": total_results,
            "results": result_data,
            "full_url_list": all_links  # Add this line to include the full URLs

        }

        return quart.Response(json.dumps(result), status=200, content_type='application/json')
    except Exception as e:
        print(f"An error occurred: {e}")
        return quart.Response(f"An error occurred: {e}", status=500)
@app.route("/Step1_make_pdf/<string:query>", methods=['GET'])
async def generate_data(query):

    data = {
    "data":query,
    "assistant_hint":"you must make a preview first of the the pdf. then you ask users if they want to process you cannot skip this step. create a menu with the option to  Preview of PDF File(P) Download pdf now (D)  Set target amount of pages(t)  (L) simply list the files created (R) rename the pds files.  Use and emojis and bold text. you cannot skip this step"

    }
    return jsonify({"data": data}), 200


from quart import request, jsonify
s3 = boto3.client('s3')


@app.route("/Step2_make_pdf", methods=['POST'])
async def save_to_s3_pdf():
    include_headers = request.args.get('include_headers', 'false').lower() == 'true'

    json_data = await request.json
    data_rows = json_data.get("data", [])

    # Create a new PDF document
    output = io.BytesIO()
    pdf = SimpleDocTemplate(output, pagesize=letter)
    styles = getSampleStyleSheet()
    content = []

    # If headers are included, they should be the first row in the data
    if include_headers:
        headers = data_rows.pop(0)
        content.append(Paragraph(headers, styles['Heading1']))

    # Add each row to the PDF document
    for row in data_rows:
        content.append(Paragraph(row, styles['BodyText']))

    pdf.build(content)

    # Reset the buffer position to the beginning
    output.seek(0)

    # Save the PDF document content to S3
    bucket_name = 'anycsv'  # Change this to your actual bucket name
    file_name = "uploaded_data.pdf"
    s3.put_object(Bucket=bucket_name, Key=file_name, Body=output.read(), ContentType='application/pdf')

    # Generate a presigned URL for the uploaded file
    url = s3.generate_presigned_url(
        'get_object',
        Params={'Bucket': bucket_name, 'Key': file_name},
        ExpiresIn=3600  # URL will be valid for 1 hour
    )
    data = {
        "role": menu,
        "assistant": url

    }

    return jsonify({"url": data})
@app.route("/get_full_abstract", methods=['POST'])
async def get_full_abstract():
    json_data = await request.json
    url = json_data.get("url")

    # Fetch the content of the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract text from the webpage
    paragraphs = soup.find_all('p')
    text_content = [p.get_text() for p in paragraphs]
    result = {
        "role": "this must scrape the url just passed in by the user, then must activate the full abstract, unless in compare mode where you summaize later",
        "role1": menu,
        "assistant": text_content

    }
    return jsonify({"data": result}), 200
@app.route("/compare", methods=['POST'])
async def compare_url():
    json_data = await request.json
    url = json_data.get("url")

    # Fetch the content of the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract text from the webpage
    paragraphs = soup.find_all('p')
    text_content = [p.get_text() for p in paragraphs]
    result = {
            "role": "this must scrape the url just passed in by the user, then must activate the full abstract, then compare both",
            "role":menu,
            "assistant":text_content


        }

    return jsonify({"data": result}), 200

if plab_wellknown == "n":
    print("default well known")
    @app.get("/.well-known/ai-plugin.json")
    async def plugin_manifest():
        host = request.headers['Host']
        with open("./.well-known/ai-plugin.json") as f:
            text = f.read()
            return quart.Response(text, mimetype="text/json")
else:
    print("plab well known")
    @app.route("/.well-known/ai-plugin.json", methods=['GET'])
    async def plugin_manifest():
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get("https://anypubmed.anygpt.ai/.well-known/ai-plugin.json")
            print(f"Request headers: {request.headers}")
            print(f"Current working directory: {os.getcwd()}")

            print(f"Received response: {response.text}")  # Print the response

            if response.status_code == 200:
                json_data = response.text  # Get the JSON as a string
                return Response(json_data, mimetype="application/json")
            else:
                return f"Failed to fetch data. Status code: {response.status_code}", 400
        except Exception as e:
            print(f"An error occurred: {e}")  # Print the exception
            return str(e), 500

@app.get("/logo.png")
async def plugin_logo():
    filename = 'logo.png'
    return await quart.send_file(filename, mimetype='image/png')






@app.get("/openapi.yaml")
async def openapi_spec():
    host = request.headers['Host']
    with open("openapi.yaml") as f:
        text = f.read()
        return quart.Response(text, mimetype="text/yaml")

if mode == "prod":
    port = int(os.environ.get("PORT", 5000))
if mode == "dev":
    port = 5003
if port == 5000:
    print("PROD PORT")
else:
    print("DEV PORT")

def main():
    app.run(debug=True, host="0.0.0.0", port=port)

if __name__ == "__main__":
    main()
